import telebot
import pymorphy2
import nltk
import config
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from deeppavlov import configs, build_model

bot = telebot.TeleBot(config.BOT_TOKEN)

tokenizer_tox = AutoTokenizer.from_pretrained("sismetanin/rubert-toxic-pikabu-2ch")
toxic_model = AutoModelForSequenceClassification.from_pretrained("sismetanin/rubert-toxic-pikabu-2ch")
ner_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)

morph = pymorphy2.MorphAnalyzer()
tokenizer = nltk.tokenize.TreebankWordTokenizer()

user_dict = {}


@bot.message_handler(commands=['start'])  # –§—É–Ω–∫—Ü–∏—è –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∫–æ–º–∞–Ω–¥—É 'start'
def start_message(message):
    bot.send_message(message.chat.id,
                     f"–ü—Ä–∏–≤–µ—Ç!\n"
                     f"‚Ä¢ –Ø –∑–Ω–∞—é <b>–ø–æ–≥–æ–¥—É</b> –≤ –ª—é–±–æ–º –≥–æ—Ä–æ–¥–µ –º–∏—Ä–∞ ‚òÄ\n"
                     f"‚Ä¢ –ü–æ–¥—Å–∫–∞–∂—É —Ç–≤–æ–π <b>–≥–æ—Ä–æ—Å–∫–æ–ø</b> üîÆ\n"
                     f"‚Ä¢ –ê —Ç–∞–∫–∂–µ –ø–æ—Å–æ–≤–µ—Ç—É—é <b>—Ä–µ—Ü–µ–ø—Ç</b> –∏–∑ –∂–µ–ª–∞–µ–º—ã—Ö –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤Ô∏èüçù\n\n"
                     f"–ß—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–º–∞–Ω–¥, –Ω–∞–ø–∏—à–∏ /help \n"
                     f"–ß—Ç–æ–±—ã –∑–∞–∫–æ–Ω—á–∏—Ç—å –¥–∏–∞–ª–æ–≥, –Ω–∞–ø–∏—à–∏ /exit\n",
                     parse_mode='HTML')


@bot.message_handler(commands=['help'])  # –§—É–Ω–∫—Ü–∏—è –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∫–æ–º–∞–Ω–¥—É 'help'
def help_message(message):
    bot.send_message(message.chat.id,
                     f"<b>–Ø –∑–Ω–∞—é —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–∞–Ω–¥—ã</b>:\n\n"
                     f"/help - <i>–ü–æ–≤—Ç–æ—Ä–∏—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ</i>\n\n"
                     f"/weather - <i>–£–∑–Ω–∞—Ç—å –ø–æ–≥–æ–¥—É</i>\n\n"
                     f"/horo - <i>–£–∑–Ω–∞—Ç—å —Å–≤–æ–π –≥–æ—Ä–æ—Å–∫–æ–ø</i>\n\n"
                     f"/cook - <i>–ü–æ–ª—É—á–∏—Ç—å —Ä–µ—Ü–µ–ø—Ç –ø–æ –∂–µ–ª–∞–µ–º—ã–º –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º</i>\n\n"
                     f"/exit - <i>–í—ã—Ö–æ–¥</i>\n",
                     parse_mode='HTML')


@bot.message_handler(commands=['exit'])  # –§—É–Ω–∫—Ü–∏—è –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∫–æ–º–∞–Ω–¥—É 'exit'
def end_message(message):
    bot.send_message(message.chat.id,
                     f"–†–∞–¥ –±—ã–ª –ø–æ–º–æ—á—å! –î–æ –≤—Å—Ç—Ä–µ—á–∏!\n")


@bot.message_handler(content_types=['text'])  # –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
def get_text(message):
    tokens_pt = tokenizer_tox(message.text, return_tensors="pt")
    with torch.no_grad():
        request_is_toxic = torch.argmax(toxic_model(**tokens_pt)[0]).item()

    if request_is_toxic:
        bot.send_message(message.chat.id,
                         text='–û—á–µ–Ω—å –≥—Ä—É–±–æ üóøüò§ –Ø –∫ —Ç–∞–∫–æ–º—É –Ω–µ –ø—Ä–∏–≤—ã–∫–ª–∞!\n\n–ß—Ç–æ–±—ã –≤—ã–∑–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∫–æ–º–∞–Ω–¥, –≤–≤–µ–¥–∏—Ç–µ /help')
        return

    request_words = tokenize_text(message.text)
    # –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

    bot.send_message(message.chat.id,
                     text='–°–∫–æ—Ä–æ –≤—Å—ë –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–æ üòè\n')
    bot.send_message(message.chat.id,
                    text='–Ø –º–æ–≥—É –µ—â–µ —á–µ–º-—Ç–æ –ø–æ–º–æ—á—å?\n–ï—Å–ª–∏ –Ω–µ—Ç, —Ç–æ –ø–æ–ø—Ä–æ—â–∞–π—Å—è —Å–æ –º–Ω–æ–π –∏–ª–∏ –Ω–∞–ø–∏—à–∏ /exit')


def tokenize_text(text):
    words = tokenizer.tokenize(text)
    result = []

    for word in words:
        p = morph.parse(word)[0]
        result.append(p.normal_form)
    return result


bot.polling(none_stop=True, interval=0)
